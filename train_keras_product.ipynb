{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "train_keras_product.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asingh-ml/NER_Product_Brand/blob/master/train_keras_product.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LunWwM2iW2TC",
        "colab_type": "code",
        "outputId": "31473617-8ffd-424d-d0db-6de895d323d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0JP0t-HW2TL",
        "colab_type": "code",
        "outputId": "b4ed377e-bbf4-4f5f-e64c-9d0273872ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRC5XavKc4DV",
        "colab_type": "code",
        "outputId": "8615eb1b-ace9-4b48-9d40-0b810954f5b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import keras\n",
        "print(keras.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTVL3pe1dQNA",
        "colab_type": "code",
        "outputId": "467021cc-e774-438f-f974-373a182d9856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "!pip install q keras==2.2.4"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting q\n",
            "  Downloading https://files.pythonhosted.org/packages/53/bc/51619d89e0bd855567e7652fa16d06f1ed36a85f108a7fe71f6629bf719d/q-2.6-py2.py3-none-any.whl\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\r\u001b[K     |█                               | 10kB 32.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 5.8MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 10.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 11.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.3.1)\n",
            "Installing collected packages: q, keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.2.4 q-2.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6tGAtsrW2TO",
        "colab_type": "code",
        "outputId": "6c77f437-cfa9-47fe-d078-cb2ce22fc94f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 5421334450639613802\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 2833628273221969977\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 8586033466305302324\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11330115994\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 2657821808315024769\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV1af7cpW2TR",
        "colab_type": "code",
        "outputId": "b5b48d65-79ee-40f6-ce4f-12e4713f9ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20_UAToMXiPO",
        "colab_type": "code",
        "outputId": "cb7434c9-a020-4a92-e606-a206eb2cb400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c2donvcY_MA",
        "colab_type": "code",
        "outputId": "14616b06-9e6b-43fe-81ee-38173a1734c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/Ner_data\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "amazon_product_unknown.h5  data_unknown_brand\t glove.6B.50d.txt\n",
            "data\t\t\t   data_unknown_product  model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYZOSuyRxusO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp \"/content/drive/My Drive/Ner_data/data/glove.6B.50d.trimmed.npz\" \"data/glove.6B.50d.trimmed.npz\"\n",
        "!cp \"/content/drive/My Drive/Ner_data/data/test_random.csv\" \"data/test_random.csv\"\n",
        "!cp \"/content/drive/My Drive/Ner_data/data/train.tagged.txt\" \"data/train.tagged.txt\"\n",
        "!cp \"/content/drive/My Drive/Ner_data/data/test.tagged.txt\" \"data/test.tagged.txt\"\n",
        "!cp \"/content/drive/My Drive/Ner_data/data/tags.txt\" \"data/tags.txt\"\n",
        "!cp \"/content/drive/My Drive/Ner_data/data/words.txt\" \"data/words.txt\"\n",
        "# !cp \"/content/drive/My Drive/Ner_data/data/test_unknown_product.csv\" \"data/test_unknown_product.csv\"\n",
        "# !cp \"/content/drive/My Drive/Ner_data/train.tagged.pro.txt\" \"data/train.tagged.txt\"\n",
        "# !cp \"/content/drive/My Drive/Ner_data/test.tagged.pro.txt\" \"data/test.tagged.txt\"\n",
        "# !cp \"/content/drive/My Drive/Ner_data/glove.6B.50d.txt\" \"data/glove/glove.6B.50d.txt\"\n",
        "# !cp \"/content/drive/My Drive/Ner_data/amazon_product_unknown.h5\" \"results/saves/amazon_product_unknown.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQkPDrNjW2TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.layers import Input, Bidirectional, LSTM, Embedding, Dense, Dropout, Lambda, Concatenate, CuDNNLSTM\n",
        "from keras.models import Model\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from model.callbacks import F1score\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from model.data_utils import minibatches, pad_sequences\n",
        "from model.crf_layer import ChainCRF\n",
        "from keras import optimizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "from model.callbacks import LossHistory"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rqf-18lkW2TX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BaseKerasModel(object):\n",
        "    \"\"\"Generic class for general methods that are not specific to NER\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Defines self.config and self.logger\n",
        "        Args:\n",
        "            config: (Config instance) class with hyper parameters,\n",
        "                vocab and embeddings\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = config.logger\n",
        "        self.model = None\n",
        "        self.conf = tf.ConfigProto()\n",
        "        self.conf.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
        "        self.conf.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
        "        self.sess = tf.Session(config=self.conf)\n",
        "        set_session(self.sess)\n",
        "        self.sess = None\n",
        "        self.saver = None\n",
        "\n",
        "    def batch_iter(self, train, batch_size, return_lengths=False):\n",
        "        \"\"\"\n",
        "        Creates a batch generator for the dataset\n",
        "        :param train: Dataset\n",
        "        :param batch_size: Batch Size\n",
        "        :param return_lengths: If True, generator returns sequence lengths. Used masking data during the evaluation step\n",
        "        :return: (number of batches in dataset, data generator)\n",
        "        \"\"\"\n",
        "        nbatches = (len(train) + batch_size - 1) // batch_size\n",
        "\n",
        "        def data_generator():\n",
        "            while True:\n",
        "                for i, (words, labels) in enumerate(minibatches(train, batch_size)):\n",
        "\n",
        "                    # word_ids = zip(*words)\n",
        "                    word_ids, sequence_lengths = pad_sequences(words, 0, max_length=self.config.max_length)\n",
        "\n",
        "                    if labels:\n",
        "                        labels, _ = pad_sequences(labels, 0, max_length=self.config.max_length)\n",
        "                        labels = [to_categorical(label, num_classes=self.config.ntags) for label in labels] # Change labels to one-hot\n",
        "\n",
        "                    # build dictionary\n",
        "                    inputs = {\n",
        "                        \"word_ids\": np.asarray(word_ids),\n",
        "                    }\n",
        "\n",
        "                    if return_lengths:\n",
        "                        yield(inputs, np.asarray(labels), sequence_lengths)\n",
        "\n",
        "                    else:\n",
        "                        yield (inputs, np.asarray(labels))\n",
        "\n",
        "        return (nbatches, data_generator())\n",
        "\n",
        "    def train(self, train, dev, show_history=False):\n",
        "        batch_size = self.config.batch_size\n",
        "\n",
        "        nbatches_train, train_generator = self.batch_iter(train, batch_size)\n",
        "        nbatches_dev, dev_generator = self.batch_iter(dev, batch_size)\n",
        "\n",
        "\n",
        "        _, f1_generator = self.batch_iter(dev, batch_size, return_lengths=True)\n",
        "        f1 = F1score(f1_generator, nbatches_dev, self.run_evaluate)\n",
        "\n",
        "        callbacks = self.gen_callbacks([f1])\n",
        "\n",
        "        history = self.model.fit_generator(generator=train_generator,\n",
        "                                           steps_per_epoch=nbatches_train,\n",
        "                                           validation_data=dev_generator,\n",
        "                                           validation_steps=nbatches_dev,\n",
        "                                           epochs=self.config.nepochs,\n",
        "                                           callbacks=callbacks) #, nbatches_train\n",
        "\n",
        "        if show_history:\n",
        "            print(history.history['f1'])\n",
        "            pass\n",
        "\n",
        "\n",
        "    def predict_words(self, words_raw):\n",
        "        words = [self.config.processing_word(w) for w in words_raw]\n",
        "        # if type(words[0]) == tuple:\n",
        "        #     words = zip(*words)\n",
        "        # char_ids, word_ids = words\n",
        "\n",
        "        word_ids = np.asarray(words)\n",
        "        s = word_ids.shape\n",
        "        word_ids = word_ids.reshape(-1, s[0])\n",
        "        inputs = [word_ids]\n",
        "\n",
        "        one_hot_preds = self.model.predict_on_batch(inputs)\n",
        "        #print(\"One hot preds: \", one_hot_preds)\n",
        "        one_hot_preds = [a.flatten() for a in one_hot_preds.squeeze()] #Squeeze to remove unnecessary 1st dimension for batch size\n",
        "        #print(\"One hot preds: \", one_hot_preds)\n",
        "\n",
        "        pred_ids = np.argmax(one_hot_preds, axis=1)\n",
        "        #print(\"Pred ids: \", pred_ids)\n",
        "\n",
        "        preds = [self.idx_to_tag[idx] for idx in pred_ids]\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def run_evaluate(self, data_generator, steps_per_epoch):\n",
        "        accs = []\n",
        "        label_true = []\n",
        "        label_pred = []\n",
        "        for i in range(steps_per_epoch):\n",
        "            #try:\n",
        "            x_true, y_true, sequence_lengths = next(data_generator)\n",
        "            y_pred = self.model.predict_on_batch(x_true)\n",
        "\n",
        "            for lab, lab_pred, length in zip(y_true, y_pred,\n",
        "                                             sequence_lengths):\n",
        "                lab = lab[:length]\n",
        "                lab_pred = lab_pred[:length]\n",
        "\n",
        "                lab = np.argmax(lab, axis=1)\n",
        "                lab_pred = np.argmax(lab_pred, axis=1)\n",
        "                accs += [a==b for (a, b) in zip(lab, lab_pred)]\n",
        "\n",
        "\n",
        "                label_true.extend(lab)\n",
        "                label_pred.extend(lab_pred)\n",
        "\n",
        "\n",
        "        label_true = np.asarray(label_true)\n",
        "        #print(\"Truths: \", label_true)\n",
        "        label_pred = np.asarray(label_pred)\n",
        "        #print(\"Preds: \", label_pred)\n",
        "\n",
        "        acc = np.mean(accs)\n",
        "        print(\"acc: \", 100*acc)\n",
        "\n",
        "        micro_score = f1_score(label_true, label_pred, average='micro')\n",
        "        print(' - micro f1: {:04.2f}'.format(micro_score * 100))\n",
        "\n",
        "        macro_score = f1_score(label_true, label_pred, average='macro')\n",
        "        print(' - macro f1: {:04.2f}'.format(macro_score * 100))\n",
        "\n",
        "        weighted_score = f1_score(label_true, label_pred, average='weighted')\n",
        "        print(' - weighted f1: {:04.2f}'.format(weighted_score * 100))\n",
        "\n",
        "        print(classification_report(label_true, label_pred))\n",
        "        return (micro_score, macro_score, weighted_score)\n",
        "\n",
        "    def get_loss(self):\n",
        "        return self._loss\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self.model, name)\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        return self._optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0oFtMOQW2Ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BLSTMCRF(BaseKerasModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BLSTMCRF, self).__init__(config)\n",
        "        self._loss = None #losses.sparse_categorical_crossentropy\n",
        "        self._optimizer = optimizers.Nadam(lr=self.config.lr)\n",
        "        self.idx_to_tag = {idx: tag for tag, idx in\n",
        "                           self.config.vocab_tags.items()}\n",
        "\n",
        "    def build(self):\n",
        "        inputs = [] #Create input for Model\n",
        "\n",
        "        # build word embeddings\n",
        "        input_words = Input(shape=(None,), dtype='int32', name='word_ids')\n",
        "        inputs.append(input_words)\n",
        "        if self.config.embeddings is None:\n",
        "            word_embeddings = Embedding(input_dim=self.config.nwords,\n",
        "                                        output_dim=self.config.dim_word,\n",
        "                                        mask_zero=True,\n",
        "                                        name=\"word_embeddings\")(input_words)\n",
        "        else:\n",
        "            word_embeddings = Embedding(input_dim=self.config.nwords,\n",
        "                                        output_dim=self.config.dim_word,\n",
        "                                        mask_zero=True,\n",
        "                                        weights=[self.config.embeddings],\n",
        "                                        trainable=self.config.train_embeddings,\n",
        "                                        name=\"word_embeddings\")(input_words)\n",
        "\n",
        "        word_embeddings = Dropout(self.config.dropout)(word_embeddings)\n",
        "        encoded_text = Bidirectional(LSTM(units=self.config.hidden_size_lstm, return_sequences=True),\n",
        "                                     name=\"bidirectional_1\")(word_embeddings)\n",
        "        encoded_text = Dropout(self.config.dropout)(encoded_text)\n",
        "        encoded_text = Bidirectional(LSTM(units=self.config.hidden_size_lstm, return_sequences=True),\n",
        "                                     name=\"bidirectional_2\")(encoded_text)\n",
        "        encoded_text = Dropout(self.config.dropout)(encoded_text)\n",
        "        encoded_text = TimeDistributed(Dense(self.config.ntags))(encoded_text)\n",
        "        encoded_text = Dropout(self.config.dropout)(encoded_text)\n",
        "        #encoded_text = Dense(100, activation='tanh')(encoded_text)\n",
        "\n",
        "        if self.config.use_crf:\n",
        "#             crf = CRF(self.config.ntags, sparse_target=False)\n",
        "            crf = ChainCRF()\n",
        "            self._loss = crf.loss\n",
        "#             self._loss = crf.loss_function\n",
        "            pred = crf(encoded_text)\n",
        "\n",
        "        else:\n",
        "            self._loss = 'categorical_crossentropy'\n",
        "            pred = Dense(self.config.ntags, activation='softmax')(encoded_text)\n",
        "\n",
        "        self.model = Model(inputs, pred)\n",
        "\n",
        "\n",
        "    def gen_callbacks(self, callbacks_list):\n",
        "        lrate = LearningRateScheduler(self.step_decay)\n",
        "        callbacks_list.append(lrate)\n",
        "\n",
        "        #loss_history = LossHistory(self.step_decay)\n",
        "        #callbacks_list.append(loss_history)\n",
        "        return callbacks_list\n",
        "\n",
        "\n",
        "    def step_decay(self, epoch):\n",
        "        initial_lrate = self.config.lr\n",
        "        decay = self.config.lr_decay\n",
        "        epochs_drop = self.config.epoch_drop\n",
        "        lrate = initial_lrate * math.pow(decay,\n",
        "                                         math.floor((1+epoch)/epochs_drop))\n",
        "        return lrate\n",
        "\n",
        "    def plot_history(self, history):\n",
        "        plt.plot(history.losses)\n",
        "        plt.title('model losses')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGBhmTMvW2Tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from model.data_utils import Dataset\n",
        "from model.config import Config\n",
        "from keras.models import load_model\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRiNC6asW2Tf",
        "colab_type": "code",
        "outputId": "0f67367d-a25d-421f-d06b-26ccf6c6df49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "# build model\n",
        "model = BLSTMCRF(config)\n",
        "model.build()\n",
        "model.compile(optimizer=model.get_optimizer(), loss=model.get_loss())  # , metrics=['categorical_accuracy']\n",
        "print(model.summary())"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
            "\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "word_ids (InputLayer)        (None, None)              0         \n",
            "_________________________________________________________________\n",
            "word_embeddings (Embedding)  (None, None, 50)          2656900   \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, None, 50)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 300)         242400    \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, None, 300)         542400    \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, None, 300)         0         \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, None, 3)           903       \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, None, 3)           0         \n",
            "_________________________________________________________________\n",
            "chain_crf_3 (ChainCRF)       (None, None, 3)           15        \n",
            "=================================================================\n",
            "Total params: 3,442,618\n",
            "Trainable params: 785,718\n",
            "Non-trainable params: 2,656,900\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQRS16toW2Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create datasets\n",
        "dev = Dataset(config.filename_dev, config.processing_word,\n",
        "              config.processing_tag, config.max_iter)\n",
        "train = Dataset(config.filename_train, config.processing_word,\n",
        "                config.processing_tag, config.max_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFMxyDFGW2Tk",
        "colab_type": "code",
        "outputId": "88afca08-236c-4316-c300-40222cf50b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# train model\n",
        "model.train(train, dev)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "3676/3676 [==============================] - 535s 146ms/step - loss: 6.1944 - val_loss: 5.1933\n",
            "acc:  91.39426232200223\n",
            " - micro f1: 91.39\n",
            " - macro f1: 85.06\n",
            " - weighted f1: 90.91\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.61      0.73    431137\n",
            "           1       0.94      0.83      0.88    508493\n",
            "           2       0.91      0.98      0.94   2653252\n",
            "\n",
            "    accuracy                           0.91   3592882\n",
            "   macro avg       0.92      0.80      0.85   3592882\n",
            "weighted avg       0.91      0.91      0.91   3592882\n",
            "\n",
            "Epoch 2/8\n",
            "3676/3676 [==============================] - 527s 143ms/step - loss: 3.9530 - val_loss: 4.6765\n",
            "acc:  93.84683382309801\n",
            " - micro f1: 93.85\n",
            " - macro f1: 89.93\n",
            " - weighted f1: 93.66\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.75      0.84    431137\n",
            "           1       0.94      0.86      0.90    508493\n",
            "           2       0.94      0.98      0.96   2653252\n",
            "\n",
            "    accuracy                           0.94   3592882\n",
            "   macro avg       0.94      0.87      0.90   3592882\n",
            "weighted avg       0.94      0.94      0.94   3592882\n",
            "\n",
            "Epoch 3/8\n",
            "3676/3676 [==============================] - 544s 148ms/step - loss: 3.6598 - val_loss: 4.5310\n",
            "acc:  94.77441786287443\n",
            " - micro f1: 94.77\n",
            " - macro f1: 91.66\n",
            " - weighted f1: 94.67\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.81      0.88    431137\n",
            "           1       0.94      0.88      0.91    508493\n",
            "           2       0.95      0.98      0.97   2653252\n",
            "\n",
            "    accuracy                           0.95   3592882\n",
            "   macro avg       0.95      0.89      0.92   3592882\n",
            "weighted avg       0.95      0.95      0.95   3592882\n",
            "\n",
            "Epoch 4/8\n",
            "3676/3676 [==============================] - 544s 148ms/step - loss: 3.5216 - val_loss: 4.4519\n",
            "acc:  95.34721708088382\n",
            " - micro f1: 95.35\n",
            " - macro f1: 92.70\n",
            " - weighted f1: 95.28\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.85      0.90    431137\n",
            "           1       0.95      0.88      0.91    508493\n",
            "           2       0.96      0.98      0.97   2653252\n",
            "\n",
            "    accuracy                           0.95   3592882\n",
            "   macro avg       0.95      0.91      0.93   3592882\n",
            "weighted avg       0.95      0.95      0.95   3592882\n",
            "\n",
            "Epoch 5/8\n",
            "3676/3676 [==============================] - 536s 146ms/step - loss: 3.4312 - val_loss: 4.3849\n",
            "acc:  95.758057180837\n",
            " - micro f1: 95.76\n",
            " - macro f1: 93.40\n",
            " - weighted f1: 95.71\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.87      0.91    431137\n",
            "           1       0.94      0.90      0.92    508493\n",
            "           2       0.96      0.98      0.97   2653252\n",
            "\n",
            "    accuracy                           0.96   3592882\n",
            "   macro avg       0.95      0.92      0.93   3592882\n",
            "weighted avg       0.96      0.96      0.96   3592882\n",
            "\n",
            "Epoch 6/8\n",
            "3676/3676 [==============================] - 558s 152ms/step - loss: 3.3722 - val_loss: 4.3811\n",
            "acc:  95.92254908455106\n",
            " - micro f1: 95.92\n",
            " - macro f1: 93.67\n",
            " - weighted f1: 95.87\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.88      0.92    431137\n",
            "           1       0.95      0.89      0.92    508493\n",
            "           2       0.96      0.98      0.97   2653252\n",
            "\n",
            "    accuracy                           0.96   3592882\n",
            "   macro avg       0.96      0.92      0.94   3592882\n",
            "weighted avg       0.96      0.96      0.96   3592882\n",
            "\n",
            "Epoch 7/8\n",
            "3676/3676 [==============================] - 547s 149ms/step - loss: 3.3253 - val_loss: 4.3654\n",
            "acc:  96.07081446036914\n",
            " - micro f1: 96.07\n",
            " - macro f1: 93.91\n",
            " - weighted f1: 96.02\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.88      0.92    431137\n",
            "           1       0.95      0.90      0.92    508493\n",
            "           2       0.96      0.99      0.97   2653252\n",
            "\n",
            "    accuracy                           0.96   3592882\n",
            "   macro avg       0.96      0.92      0.94   3592882\n",
            "weighted avg       0.96      0.96      0.96   3592882\n",
            "\n",
            "Epoch 8/8\n",
            "3676/3676 [==============================] - 550s 150ms/step - loss: 3.2847 - val_loss: 4.3485\n",
            "acc:  96.19066253776217\n",
            " - micro f1: 96.19\n",
            " - macro f1: 94.11\n",
            " - weighted f1: 96.15\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.89      0.92    431137\n",
            "           1       0.95      0.90      0.92    508493\n",
            "           2       0.96      0.99      0.97   2653252\n",
            "\n",
            "    accuracy                           0.96   3592882\n",
            "   macro avg       0.96      0.92      0.94   3592882\n",
            "weighted avg       0.96      0.96      0.96   3592882\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4F9V3S9W2Tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvu8uzd5W2Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create directory for storing model object\n",
        "if not os.path.exists('./results/saves'):\n",
        "    os.makedirs('./results/saves')\n",
        "\n",
        "# Save model\n",
        "model.save_weights('./results/saves/amazon_unknown_brand_product.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA9X2IylW2Ts",
        "colab_type": "code",
        "outputId": "63917871-c957-40da-c14a-5345da63ec9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Model Trained and saved\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Trained and saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc97PfW9W2Tw",
        "colab_type": "code",
        "outputId": "26579f96-f14b-47b5-ac57-58d0e244f088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "config = Config()\n",
        "\n",
        "model = BLSTMCRF(config) #Word_BLSTM(config)\n",
        "\n",
        "model.build()\n",
        "model.compile(optimizer=model.get_optimizer(), loss=model.get_loss()) #, metrics=['acc']\n",
        "\n",
        "model.load_weights('./results/saves/amazon_product_unknown.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVY6k1e46AGv",
        "colab_type": "code",
        "outputId": "5330db6a-ec6c-41b0-b083-fb8e5be9fb85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.BLSTMCRF at 0x7f50eed8c710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzewY5cz7QQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "4e8dbfb0-1dda-40d3-f3df-d05e66524c49"
      },
      "source": [
        "test = Dataset(config.filename_test, config.processing_word,\n",
        "                        config.processing_tag, config.max_iter)\n",
        "\n",
        "batch_size = config.batch_size\n",
        "nbatches_test, test_generator = model.batch_iter(test, batch_size, return_lengths=True)\n",
        "\n",
        "model.run_evaluate(test_generator, nbatches_test)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc:  84.87002861265077\n",
            " - micro f1: 84.87\n",
            " - macro f1: 75.47\n",
            " - weighted f1: 83.43\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.45      0.58    128002\n",
            "           1       0.92      0.67      0.77    132973\n",
            "           2       0.84      0.98      0.91    562437\n",
            "\n",
            "    accuracy                           0.85    823412\n",
            "   macro avg       0.86      0.70      0.75    823412\n",
            "weighted avg       0.85      0.85      0.83    823412\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8487002861265077, 0.7547080830591738, 0.8343335789052635)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekLcV0U5w8Gg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rreplace(s, old, new, occurrence):\n",
        "    li = s.rsplit(old, occurrence)\n",
        "    return new.join(li)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rtARG8gw9dI",
        "colab_type": "code",
        "outputId": "b8302e1f-99fa-42f8-ee87-87af41cdc702",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "! pip install seqeval"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.3)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=25649bb0d18a5b15b1ac2758d23d2d091a36eaa1f07f5116065d5a9efac27e80\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adeFLwV6wdxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from seqeval.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB9zqLFYW2T3",
        "colab_type": "code",
        "outputId": "ee4d0057-284e-4c9d-9a8a-3f568a7efba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "testdata = pd.read_csv('./data/test_unknown_brand.csv')\n",
        "testdata['tag'] = testdata['tag'].apply(lambda x: literal_eval(x))\n",
        "actual = []\n",
        "predicted = []\n",
        "count = 0\n",
        "for idx, row in testdata.iterrows():\n",
        "    count+=1\n",
        "    tagged = row['tag']\n",
        "    words, labels = [word for word, ta in tagged], [ta for word, ta in tagged]\n",
        "    if not words or len(tagged) == 1:\n",
        "        continue\n",
        "    pred = model.predict_words(words)\n",
        "    # print(words)\n",
        "    # print(labels)\n",
        "    # print(pred)\n",
        "    labels = rreplace(' '.join(items for items in labels).replace('PRODUCT', 'B-PRODUCT', 1), 'PRODUCT',\n",
        "                      'I-PRODUCT',\n",
        "                      ' '.join(items for items in labels).replace('PRODUCT', 'B-PRODUCT', 1).count(\n",
        "                          'PRODUCT') - 1).split()\n",
        "    labels = rreplace(' '.join(items for items in labels).replace('BRAND', 'B-BRAND', 1), 'BRAND',\n",
        "                          'I-BRAND',\n",
        "                          ' '.join(items for items in labels).replace('BRAND', 'B-BRAND', 1).count(\n",
        "                              'BRAND') - 1).split()\n",
        "\n",
        "    pred = rreplace(' '.join(items for items in pred).replace('PRODUCT', 'B-PRODUCT', 1), 'PRODUCT',\n",
        "                      'I-PRODUCT',\n",
        "                      ' '.join(items for items in pred).replace('PRODUCT', 'B-PRODUCT', 1).count(\n",
        "                          'PRODUCT') - 1).split()\n",
        "    pred = rreplace(' '.join(items for items in pred).replace('BRAND', 'B-BRAND', 1), 'BRAND',\n",
        "                          'I-BRAND',\n",
        "                          ' '.join(items for items in pred).replace('BRAND', 'B-BRAND', 1).count(\n",
        "                              'BRAND') - 1).split()\n",
        "\n",
        "\n",
        "    # print(words)\n",
        "    # print(labels)\n",
        "    # print(pred)\n",
        "    actual.append(labels)\n",
        "    predicted.append(pred)\n",
        "    if count % 10000 == 0:\n",
        "        print(count, \" Records processed\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000  Records processed\n",
            "20000  Records processed\n",
            "30000  Records processed\n",
            "40000  Records processed\n",
            "50000  Records processed\n",
            "60000  Records processed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuDi0oGN1-Ns",
        "colab_type": "code",
        "outputId": "87b8e605-0dc3-41b9-8aa5-e925330e9185",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Random Test Data results\n",
        "print(classification_report(actual, predicted))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           precision    recall  f1-score   support\n",
            "\n",
            "  PRODUCT       0.87      0.62      0.73     91891\n",
            "    BRAND       0.86      0.85      0.85     93730\n",
            "\n",
            "micro avg       0.87      0.74      0.80    185621\n",
            "macro avg       0.87      0.74      0.79    185621\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS96hFpMWvts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "03e24f54-0c75-4d77-f61c-bd385e5ff7d6"
      },
      "source": [
        "# Unknown Product Test Data results\n",
        "print(classification_report(actual, predicted))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           precision    recall  f1-score   support\n",
            "\n",
            "  PRODUCT       0.17      0.07      0.10     27005\n",
            "    BRAND       0.06      0.78      0.11      2018\n",
            "\n",
            "micro avg       0.09      0.12      0.10     29023\n",
            "macro avg       0.16      0.12      0.10     29023\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omrRYgpwcCxm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "0a499b45-3140-4f68-dddc-123c948152d3"
      },
      "source": [
        "# Unknown Brand Test Data results\n",
        "print(classification_report(actual, predicted))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           precision    recall  f1-score   support\n",
            "\n",
            "  PRODUCT       0.56      0.32      0.40     93416\n",
            "    BRAND       0.56      0.55      0.55     65597\n",
            "\n",
            "micro avg       0.56      0.41      0.47    159013\n",
            "macro avg       0.56      0.41      0.47    159013\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YAqa4J2sjvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}